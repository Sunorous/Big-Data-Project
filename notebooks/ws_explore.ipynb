{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b97d4e2-7ce9-4faf-96d4-446d30c7ae78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sodapy in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: requests>=2.28.1 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from sodapy) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests>=2.28.1->sodapy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests>=2.28.1->sodapy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests>=2.28.1->sodapy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from requests>=2.28.1->sodapy) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sodapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bc09eff-68d6-4ec2-b4a4-f030c7fc03f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (2023.4.1)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask) (2023.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask) (23.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask) (6.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from click>=8.0->dask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from importlib-metadata>=4.13.0->dask) (3.15.0)\n",
      "Requirement already satisfied: locket in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from partd>=1.2.0->dask) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "522d1a52-c860-43e7-8a52-7df149ea51f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask-glm\n",
      "  Downloading dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask-glm) (2.2.1)\n",
      "Requirement already satisfied: dask[array] in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask-glm) (2023.4.1)\n",
      "Collecting multipledispatch>=0.4.9 (from dask-glm)\n",
      "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask-glm) (1.10.1)\n",
      "Collecting scikit-learn>=0.18 (from dask-glm)\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-win_amd64.whl (8.3 MB)\n",
      "                                              0.0/8.3 MB ? eta -:--:--\n",
      "     ---                                      0.7/8.3 MB 23.1 MB/s eta 0:00:01\n",
      "     -------------                            2.7/8.3 MB 34.5 MB/s eta 0:00:01\n",
      "     ---------------------                    4.5/8.3 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------         6.8/8.3 MB 39.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.3/8.3 MB 37.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.3/8.3 MB 33.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from multipledispatch>=0.4.9->dask-glm) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn>=0.18->dask-glm) (1.24.3)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn>=0.18->dask-glm)\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "                                              0.0/298.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 298.0/298.0 kB 18.0 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.18->dask-glm)\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array]->dask-glm) (8.1.3)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array]->dask-glm) (2023.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array]->dask-glm) (23.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array]->dask-glm) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array]->dask-glm) (6.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array]->dask-glm) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array]->dask-glm) (6.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from click>=8.0->dask[array]->dask-glm) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from importlib-metadata>=4.13.0->dask[array]->dask-glm) (3.15.0)\n",
      "Requirement already satisfied: locket in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from partd>=1.2.0->dask[array]->dask-glm) (1.0.0)\n",
      "Installing collected packages: threadpoolctl, multipledispatch, joblib, scikit-learn, dask-glm\n",
      "Successfully installed dask-glm-0.2.0 joblib-1.2.0 multipledispatch-0.6.0 scikit-learn-1.2.2 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dask-glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c809837-fa09-487d-a3f8-0833b2d128a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask_ml\n",
      "  Downloading dask_ml-2023.3.24-py3-none-any.whl (148 kB)\n",
      "                                              0.0/148.7 kB ? eta -:--:--\n",
      "     ----------                            41.0/148.7 kB 991.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 148.7/148.7 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: dask[array,dataframe]>=2.4.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask_ml) (2023.4.1)\n",
      "Collecting distributed>=2.4.0 (from dask_ml)\n",
      "  Downloading distributed-2023.4.1-py3-none-any.whl (962 kB)\n",
      "                                              0.0/962.3 kB ? eta -:--:--\n",
      "     ------------------------------------- 962.3/962.3 kB 30.7 MB/s eta 0:00:00\n",
      "Collecting numba>=0.51.0 (from dask_ml)\n",
      "  Downloading numba-0.57.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "                                              0.0/2.6 MB ? eta -:--:--\n",
      "     ------------------------                 1.6/2.6 MB 33.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 32.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask_ml) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask_ml) (2.0.0)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask_ml) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask_ml) (1.10.1)\n",
      "Requirement already satisfied: dask-glm>=0.2.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask_ml) (0.2.0)\n",
      "Requirement already satisfied: multipledispatch>=0.4.9 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask_ml) (0.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask_ml) (23.1)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask-glm>=0.2.0->dask_ml) (2.2.1)\n",
      "Requirement already satisfied: click>=8.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (8.1.3)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (2023.5.0)\n",
      "Requirement already satisfied: partd>=1.2.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (1.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (6.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from dask[array,dataframe]>=2.4.0->dask_ml) (6.6.0)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from distributed>=2.4.0->dask_ml) (3.1.2)\n",
      "Requirement already satisfied: locket>=1.0.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from distributed>=2.4.0->dask_ml) (1.0.0)\n",
      "Collecting msgpack>=1.0.0 (from distributed>=2.4.0->dask_ml)\n",
      "  Downloading msgpack-1.0.5-cp38-cp38-win_amd64.whl (62 kB)\n",
      "                                              0.0/62.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 62.5/62.5 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: psutil>=5.7.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from distributed>=2.4.0->dask_ml) (5.9.5)\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed>=2.4.0->dask_ml)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting tblib>=1.6.0 (from distributed>=2.4.0->dask_ml)\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tornado>=6.0.3 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from distributed>=2.4.0->dask_ml) (6.3)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from distributed>=2.4.0->dask_ml) (1.26.15)\n",
      "Collecting zict>=2.2.0 (from distributed>=2.4.0->dask_ml)\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "                                              0.0/43.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.3/43.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: six in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from multipledispatch>=0.4.9->dask_ml) (1.16.0)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba>=0.51.0->dask_ml)\n",
      "  Downloading llvmlite-0.40.0-cp38-cp38-win_amd64.whl (27.7 MB)\n",
      "                                              0.0/27.7 MB ? eta -:--:--\n",
      "     --                                       1.5/27.7 MB 31.8 MB/s eta 0:00:01\n",
      "     -----                                    3.5/27.7 MB 37.5 MB/s eta 0:00:01\n",
      "     --------                                 5.6/27.7 MB 39.4 MB/s eta 0:00:01\n",
      "     -----------                              7.6/27.7 MB 40.6 MB/s eta 0:00:01\n",
      "     -------------                            9.6/27.7 MB 43.7 MB/s eta 0:00:01\n",
      "     ----------------                        11.6/27.7 MB 43.7 MB/s eta 0:00:01\n",
      "     -------------------                     13.7/27.7 MB 40.9 MB/s eta 0:00:01\n",
      "     ----------------------                  15.7/27.7 MB 40.9 MB/s eta 0:00:01\n",
      "     -------------------------               17.8/27.7 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------------             19.8/27.7 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------------------          21.8/27.7 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------------------       23.9/27.7 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------------------------    25.9/27.7 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  27.7/27.7 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  27.7/27.7 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 27.7/27.7 MB 29.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from pandas>=0.24.2->dask_ml) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from pandas>=0.24.2->dask_ml) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from pandas>=0.24.2->dask_ml) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn>=1.2.0->dask_ml) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from scikit-learn>=1.2.0->dask_ml) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from click>=8.0->dask[array,dataframe]>=2.4.0->dask_ml) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from importlib-metadata>=4.13.0->dask[array,dataframe]>=2.4.0->dask_ml) (3.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\16142\\appdata\\roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask_ml) (2.1.2)\n",
      "Installing collected packages: sortedcontainers, msgpack, zict, tblib, llvmlite, numba, distributed, dask_ml\n",
      "Successfully installed dask_ml-2023.3.24 distributed-2023.4.1 llvmlite-0.40.0 msgpack-1.0.5 numba-0.57.0 sortedcontainers-2.4.0 tblib-1.7.0 zict-3.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dask_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083a55fc-ba8b-4b91-93ae-8a5935a63a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "import pyspark\n",
    "import json\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask_glm.datasets import make_regression\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "\n",
    "from pyspark.sql.functions import col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e96fc81-dca1-4795-bfb7-eeb4490eec39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test_data_path = '/Users/maludee/repos/Big-Data-Project/data/Parking_Violations_Issued_-_Fiscal_Year_2023.csv'\n",
    "#open_data_path = '/Users/maludee/repos/Big-Data-Project/data/Open_Parking_and_Camera_Violations.csv'\n",
    "#precinct_data_path = \"/Users/maludee/repos/Big-Data-Project/data/nypp.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d29a6-7b2b-4fa1-bce7-fd39c6633968",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Precinct Map for Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "354c55b6-c824-4354-8afe-8c83bbe108dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/05 23:11:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# YEARLY PARKING DATA - FOR TABLEAU\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(test_data_path)\n",
    "\n",
    "df.repartition(1).write.mode('overwrite').parquet('tmp/parking_2023')\n",
    "\n",
    "df = spark.read.parquet('/Users/maludee/repos/Big-Data-Project/notebooks/tmp/parking_2023/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe91273-e265-4ad8-9499-792001b9bb81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons Number: long (nullable = true)\n",
      " |-- Plate ID: string (nullable = true)\n",
      " |-- Registration State: string (nullable = true)\n",
      " |-- Plate Type: string (nullable = true)\n",
      " |-- Issue Date: string (nullable = true)\n",
      " |-- Violation Code: integer (nullable = true)\n",
      " |-- Vehicle Body Type: string (nullable = true)\n",
      " |-- Vehicle Make: string (nullable = true)\n",
      " |-- Issuing Agency: string (nullable = true)\n",
      " |-- Street Code1: integer (nullable = true)\n",
      " |-- Street Code2: integer (nullable = true)\n",
      " |-- Street Code3: integer (nullable = true)\n",
      " |-- Vehicle Expiration Date: integer (nullable = true)\n",
      " |-- Violation Location: integer (nullable = true)\n",
      " |-- Violation Precinct: integer (nullable = true)\n",
      " |-- Issuer Precinct: integer (nullable = true)\n",
      " |-- Issuer Code: integer (nullable = true)\n",
      " |-- Issuer Command: string (nullable = true)\n",
      " |-- Issuer Squad: string (nullable = true)\n",
      " |-- Violation Time: string (nullable = true)\n",
      " |-- Time First Observed: string (nullable = true)\n",
      " |-- Violation County: string (nullable = true)\n",
      " |-- Violation In Front Of Or Opposite: string (nullable = true)\n",
      " |-- House Number: string (nullable = true)\n",
      " |-- Street Name: string (nullable = true)\n",
      " |-- Intersecting Street: string (nullable = true)\n",
      " |-- Date First Observed: integer (nullable = true)\n",
      " |-- Law Section: integer (nullable = true)\n",
      " |-- Sub Division: string (nullable = true)\n",
      " |-- Violation Legal Code: string (nullable = true)\n",
      " |-- Days Parking In Effect    : string (nullable = true)\n",
      " |-- From Hours In Effect: string (nullable = true)\n",
      " |-- To Hours In Effect: string (nullable = true)\n",
      " |-- Vehicle Color: string (nullable = true)\n",
      " |-- Unregistered Vehicle?: integer (nullable = true)\n",
      " |-- Vehicle Year: integer (nullable = true)\n",
      " |-- Meter Number: string (nullable = true)\n",
      " |-- Feet From Curb: integer (nullable = true)\n",
      " |-- Violation Post Code: string (nullable = true)\n",
      " |-- Violation Description: string (nullable = true)\n",
      " |-- No Standing or Stopping Violation: string (nullable = true)\n",
      " |-- Hydrant Violation: string (nullable = true)\n",
      " |-- Double Parking Violation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f334a4b6-0631-4b98-8a6f-574e7db823ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------------+---------------+\n",
      "|            the_geom|Precinct|   Shape_Leng|     Shape_Area|\n",
      "+--------------------+--------+-------------+---------------+\n",
      "|MULTIPOLYGON (((-...|       1|80283.5387782|4.72864229826E7|\n",
      "|MULTIPOLYGON (((-...|       5|18807.1249114|1.80945274385E7|\n",
      "|MULTIPOLYGON (((-...|       6|24875.9642171|2.20179465474E7|\n",
      "|MULTIPOLYGON (((-...|       7|17287.5444926| 1.8366669928E7|\n",
      "|MULTIPOLYGON (((-...|       9|19772.5107407|2.13953862669E7|\n",
      "+--------------------+--------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precinct_keys = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(precinct_data_path)\n",
    "precinct_keys.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38157b7f-6b22-4da3-be1b-ba1f8d276136",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+\n",
      "|violation_precinct|count_violations|\n",
      "+------------------+----------------+\n",
      "|                 0|         5938384|\n",
      "|                19|          331642|\n",
      "|                13|          291119|\n",
      "|                 6|          259154|\n",
      "|               114|          254978|\n",
      "|                14|          220291|\n",
      "+------------------+----------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use sql syntax to parse the nested json\n",
    "df.createOrReplaceTempView(\"2023_violations\")\n",
    "query = \"\"\"\n",
    "select \n",
    "    `Violation Precinct` as violation_precinct,\n",
    "    count(*) as count_violations\n",
    "from \n",
    "    2023_violations\n",
    "group by 1\n",
    "order by 2 desc\n",
    "\n",
    "\"\"\"\n",
    "precincts = spark.sql(query)\n",
    "precincts.toPandas().to_csv('violations_by_precinct.csv')\n",
    "\n",
    "# I think precinct 0 is in error and should be ignored\n",
    "precincts.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6ddbc-1c30-4f62-9508-ea77c2be6da4",
   "metadata": {},
   "source": [
    "# ML Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42843ac5-f993-4cc3-aa32-b972da54fed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPEN DATASET - FOR MACHINE LEARNING \n",
    "df = spark.read.option(\"inferSchema\", \"true\")\\\n",
    "                  .option(\"header\", \"true\")\\\n",
    "                  .csv(\"Open_Parking_and_Camera_Violations.csv\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857d9047-83f1-4ef3-9fe1-d1b1ee4e621f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Plate: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- License Type: string (nullable = true)\n",
      " |-- Summons Number: long (nullable = true)\n",
      " |-- Issue Date: string (nullable = true)\n",
      " |-- Violation Time: string (nullable = true)\n",
      " |-- Violation: string (nullable = true)\n",
      " |-- Judgment Entry Date: string (nullable = true)\n",
      " |-- Fine Amount: double (nullable = true)\n",
      " |-- Penalty Amount: double (nullable = true)\n",
      " |-- Interest Amount: double (nullable = true)\n",
      " |-- Reduction Amount: double (nullable = true)\n",
      " |-- Payment Amount: double (nullable = true)\n",
      " |-- Amount Due: double (nullable = true)\n",
      " |-- Precinct: integer (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- Issuing Agency: string (nullable = true)\n",
      " |-- Violation Status: string (nullable = true)\n",
      " |-- Summons Image: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af4dd724-4ed0-474c-ad9d-abdbc05b2c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select a subset of columns for the model (TODO: add more)\n",
    "training_df = df.na.drop().select([col(\"Violation\"), col(\"Fine Amount\"), col(\"Precinct\")])\n",
    "training_df = training_df.filter((col(\"Precinct\") >= \"1\") & (col(\"Precinct\") <= \"124\")) #filter out the invalid precints \n",
    "\n",
    "#county_df = training_df.groupBy(\"Precinct\").count()\n",
    "#county_df.toPandas().to_csv(\"county.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09c5db71-463f-4237-8c3a-72559adf72d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vio_df = training_df.groupBy(\"Violation\").count().sort(\"count\", ascending=False)\n",
    "vio_df.toPandas().to_csv(\"violation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb2d2549-9182-460e-a618-61ca85b9f7b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv('violation.csv')\n",
    "\n",
    "# Replace spaces and \"-\" with \"_\"\n",
    "df.columns = df.columns.str.replace(' ', '_').str.replace('-', '_')\n",
    "\n",
    "# Define function to convert Violation column to a case statement\n",
    "def convert_violation_text(text):\n",
    "    return \"case when Violation = '{}' then 1 else 0 end as {},\".format(text, text.replace(' ', '_').replace('-', '_'))\n",
    "\n",
    "# Create a new column with the case statements\n",
    "df['converted'] = df['Violation'].apply(convert_violation_text)\n",
    "\n",
    "# Save output as CSV file\n",
    "df[['converted']].to_csv('sql_vio.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6806899-2458-4897-9d58-67f47b5a5b19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------+------+-------------+--------------------------+----------------------------+------------------------------+---------------------------+------------+---------------------------+--------------------------+--------------+------------------+---------------------------+--------------------+---------------------------+-----------------------------+-----------------------------+---------+----------------------------+----------------------------+--------+---------+----------------------+-----------------------------+-----------+--------------------+---------------------------+------------------------------+------------------------------+----------------------------+-----------------------------+--------------------+-----------------------------+----------------------+----------------------------+----------------------+---------------------------+---------------+-------------------------+---------+------------------------------+-------------+-------------------------+----------------------------+--------------------+-------------+---------------------+-----------------------------+------------+-------------------+----------------------------+-----------------+-------------+------------------------------+----------------+-----------------------+------------------------+---------------------+-----+------------+------+-----------------------------+-----------------------------+------------+----------------------+-----------------------------+-----------------------------+------------------------------+------------------------------+---------------------------+------------------------------+-----------------------------+---------------+------------------------------+------------------------------+-----------------------------+------------------------------+------------------------------+--------------------------+-----------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+----------+-------------------+----------------------------+-----------------------------+---------------------------+-----------------+------------------+------------------------------+--------------------------+--------------------------+------------------------+--------------------+-----------+\n",
      "|Manhattan|Bronx|Brooklyn|Queens|Staten_Island|NO_PARKING_STREET_CLEANING|INSP_STICKER_EXPIRED_MISSING|FAIL_TO_DSPLY_MUNI_METER_RECPT|NO_STANDING_DAY_TIME_LIMITS|FIRE_HYDRANT|REG_STICKER_EXPIRED_MISSING|NO_PARKING_DAY_TIME_LIMITS|DOUBLE_PARKING|EXPIRED_MUNI_METER|FRONT_OR_BACK_PLATE_MISSING|NO_STANDING_BUS_STOP|NO_STANDING_COMM_METER_ZONE|FAIL_TO_DISP_MUNI_METER_RECPT|NO_STANDING_EXC_TRUCK_LOADING|CROSSWALK|NO_STANDING_EXC_AUTH_VEHICLE|DOUBLE_PARKING_MIDTOWN_COMML|SIDEWALK|BIKE_LANE|NO_MATCH_PLATE_STICKER|PLTFRM_LFTS_LWRD_POS_COMM_VEH|SAFETY_ZONE|OBSTRUCTING_DRIVEWAY|NO_STOPPING_DAY_TIME_LIMITS|COMML_PLATES_UNALTERED_VEHICLE|NGHT_PKG_ON_RESID_STR_COMM_VEH|EXPIRED_MUNI_MTR_COMM_MTR_ZN|OBSTRUCTING_TRAFFIC_INTERSECT|NO_STANDING_BUS_LANE|INSP_STICKER_MUTILATED_C_FEIT|STORAGE_3HR_COMMERCIAL|NON_COMPLIANCE_W_POSTED_SIGN|NO_STANDING_TAXI_STAND|NO_PARKING_EXC_AUTH_VEHICLE|PEDESTRIAN_RAMP|PARKED_BUS_EXC_DESIG_AREA|WRONG_WAY|SELLING_OFFERING_MCHNDSE_METER|EXPIRED_METER|NO_STANDING_HOTEL_LOADING|REG_STICKER_MUTILATED_C_FEIT|NO_STANDING_EXCP_D_S|ANGLE_PARKING|IMPROPER_REGISTRATION|NO_PARKING_EXC_HNDICAP_PERMIT|TRAFFIC_LANE|BEYOND_MARKED_SPACE|NO_PARKING_EXC_HOTEL_LOADING|MISSING_EQUIPMENT|FEEDING_METER|OVERTIME_PKG_TIME_LIMIT_POSTED|DETACHED_TRAILER|TUNNEL_ELEVATED_ROADWAY|UNAUTHORIZED_BUS_LAYOVER|NO_PARKING_TAXI_STAND|OTHER|VIN_OBSCURED|IDLING|NO_STD_EXC_TRKS_GMTDST_NO_TRK|OVERNIGHT_TRACTOR_TRAILER_PKG|INTERSECTION|UNALTERED_COMM_VEHICLE|FAILURE_TO_DISPLAY_BUS_PERMIT|NO_STANDING_FOR_HIRE_VEH_STOP|BUS_PARKING_IN_LOWER_MANHATTAN|UNAUTHORIZED_PASSENGER_PICK_UP|NIGHTTIME_STD_PKG_IN_A_PARK|EXCAVATION_VEHICLE_OBSTR_TRAFF|NO_STANDING_COMMUTER_VAN_STOP|DIVIDED_HIGHWAY|NO_STOP_STANDNG_EXCEPT_PAS_P_U|OT_PARKING_MISSING_BROKEN_METR|VEHICLE_FOR_SALE_DEALERS_ONLY|NO_OPERATOR_NAM_ADD_PH_DISPLAY|ELEVATED_DIVIDED_HIGHWAY_TUNNL|ANGLE_PARKING_COMM_VEHICLE|RAILROAD_CROSSING|EXPIRED_METER_COMM_METER_ZONE|ALTERING_INTERCITY_BUS_PERMIT|WASH_REPAIR_VEHCL_REPAIR_ONLY|PKG_IN_EXC_OF_LIM_COMM_MTR_ZN|VACANT_LOT|NO_STANDING_EXCP_DP|MIDTOWN_PKG_OR_STD_3HR_LIMIT|VEH_SALE_WSHNG_RPRNG_DRIVEWAY|MARGINAL_STREET_WATER_FRONT|OVERTIME_STDG_D_S|BUS_LANE_VIOLATION|UNALTERED_COMM_VEH_NME_ADDRESS|NO_STANDING_OFF_STREET_LOT|NO_STANDING_SNOW_EMERGENCY|REMOVE_REPLACE_FLAT_TIRE|OVERTIME_STANDING_DP|fine_amount|\n",
      "+---------+-----+--------+------+-------------+--------------------------+----------------------------+------------------------------+---------------------------+------------+---------------------------+--------------------------+--------------+------------------+---------------------------+--------------------+---------------------------+-----------------------------+-----------------------------+---------+----------------------------+----------------------------+--------+---------+----------------------+-----------------------------+-----------+--------------------+---------------------------+------------------------------+------------------------------+----------------------------+-----------------------------+--------------------+-----------------------------+----------------------+----------------------------+----------------------+---------------------------+---------------+-------------------------+---------+------------------------------+-------------+-------------------------+----------------------------+--------------------+-------------+---------------------+-----------------------------+------------+-------------------+----------------------------+-----------------+-------------+------------------------------+----------------+-----------------------+------------------------+---------------------+-----+------------+------+-----------------------------+-----------------------------+------------+----------------------+-----------------------------+-----------------------------+------------------------------+------------------------------+---------------------------+------------------------------+-----------------------------+---------------+------------------------------+------------------------------+-----------------------------+------------------------------+------------------------------+--------------------------+-----------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+----------+-------------------+----------------------------+-----------------------------+---------------------------+-----------------+------------------+------------------------------+--------------------------+--------------------------+------------------------+--------------------+-----------+\n",
      "|        1|    0|       0|     0|            0|                         0|                           0|                             0|                          0|           0|                          0|                         0|             0|                 0|                          0|                   0|                          0|                            0|                            0|        0|                           0|                           0|       0|        0|                     0|                            0|          0|                   0|                          1|                             0|                             0|                           0|                            0|                   0|                            0|                     0|                           0|                     0|                          0|              0|                        0|        0|                             0|            0|                        0|                           0|                   0|            0|                    0|                            0|           0|                  0|                           0|                0|            0|                             0|               0|                      0|                       0|                    0|    0|           0|     0|                            0|                            0|           0|                     0|                            0|                            0|                             0|                             0|                          0|                             0|                            0|              0|                             0|                             0|                            0|                             0|                             0|                         0|                0|                            0|                            0|                            0|                            0|         0|                  0|                           0|                            0|                          0|                0|                 0|                             0|                         0|                         0|                       0|                   0|      115.0|\n",
      "+---------+-----+--------+------+-------------+--------------------------+----------------------------+------------------------------+---------------------------+------------+---------------------------+--------------------------+--------------+------------------+---------------------------+--------------------+---------------------------+-----------------------------+-----------------------------+---------+----------------------------+----------------------------+--------+---------+----------------------+-----------------------------+-----------+--------------------+---------------------------+------------------------------+------------------------------+----------------------------+-----------------------------+--------------------+-----------------------------+----------------------+----------------------------+----------------------+---------------------------+---------------+-------------------------+---------+------------------------------+-------------+-------------------------+----------------------------+--------------------+-------------+---------------------+-----------------------------+------------+-------------------+----------------------------+-----------------+-------------+------------------------------+----------------+-----------------------+------------------------+---------------------+-----+------------+------+-----------------------------+-----------------------------+------------+----------------------+-----------------------------+-----------------------------+------------------------------+------------------------------+---------------------------+------------------------------+-----------------------------+---------------+------------------------------+------------------------------+-----------------------------+------------------------------+------------------------------+--------------------------+-----------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+----------+-------------------+----------------------------+-----------------------------+---------------------------+-----------------+------------------+------------------------------+--------------------------+--------------------------+------------------------+--------------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# make into a temp sql view for convenience \n",
    "training_df.createOrReplaceTempView(\"open_violations\")\n",
    "\n",
    "query = \"\"\"\n",
    "select \n",
    "    \n",
    "    case when Precinct >= 1 AND Precinct <= 40 then 1 else 0 end as Manhattan,\n",
    "    case when Precinct >= 40 AND Precinct <= 60 then 1 else 0 end as Bronx,\n",
    "    case when Precinct >= 60 AND Precinct <= 100 then 1 else 0 end as Brooklyn,\n",
    "    case when Precinct >= 100 AND Precinct <= 120 then 1 else 0 end as Queens,\n",
    "    case when Precinct >= 120 AND Precinct <= 125 then 1 else 0 end as Staten_Island,\n",
    "    \n",
    "    case when Violation = 'NO PARKING-STREET CLEANING' then 1 else 0 end as NO_PARKING_STREET_CLEANING,\n",
    "    case when Violation = 'INSP. STICKER-EXPIRED/MISSING' then 1 else 0 end as INSP_STICKER_EXPIRED_MISSING,\n",
    "    case when Violation = 'FAIL TO DSPLY MUNI METER RECPT' then 1 else 0 end as FAIL_TO_DSPLY_MUNI_METER_RECPT,\n",
    "    case when Violation = 'NO STANDING-DAY/TIME LIMITS' then 1 else 0 end as NO_STANDING_DAY_TIME_LIMITS,\n",
    "    case when Violation = 'FIRE HYDRANT' then 1 else 0 end as FIRE_HYDRANT,\n",
    "    case when Violation = 'REG. STICKER-EXPIRED/MISSING' then 1 else 0 end as REG_STICKER_EXPIRED_MISSING,\n",
    "    case when Violation = 'NO PARKING-DAY/TIME LIMITS' then 1 else 0 end as NO_PARKING_DAY_TIME_LIMITS,\n",
    "    case when Violation = 'DOUBLE PARKING' then 1 else 0 end as DOUBLE_PARKING,\n",
    "    case when Violation = 'EXPIRED MUNI METER' then 1 else 0 end as EXPIRED_MUNI_METER,\n",
    "    case when Violation = 'FRONT OR BACK PLATE MISSING' then 1 else 0 end as FRONT_OR_BACK_PLATE_MISSING,\n",
    "    case when Violation = 'NO STANDING-BUS STOP' then 1 else 0 end as NO_STANDING_BUS_STOP,\n",
    "    case when Violation = 'NO STANDING-COMM METER ZONE' then 1 else 0 end as NO_STANDING_COMM_METER_ZONE,\n",
    "    case when Violation = 'FAIL TO DISP. MUNI METER RECPT' then 1 else 0 end as FAIL_TO_DISP_MUNI_METER_RECPT,\n",
    "    case when Violation = 'NO STANDING-EXC. TRUCK LOADING' then 1 else 0 end as NO_STANDING_EXC_TRUCK_LOADING,\n",
    "    case when Violation = 'CROSSWALK' then 1 else 0 end as CROSSWALK,\n",
    "    case when Violation = 'NO STANDING-EXC. AUTH. VEHICLE' then 1 else 0 end as NO_STANDING_EXC_AUTH_VEHICLE,\n",
    "    case when Violation = 'DOUBLE PARKING-MIDTOWN COMML' then 1 else 0 end as DOUBLE_PARKING_MIDTOWN_COMML,\n",
    "    case when Violation = 'SIDEWALK' then 1 else 0 end as SIDEWALK,\n",
    "    case when Violation = 'BIKE LANE' then 1 else 0 end as BIKE_LANE,\n",
    "    case when Violation = 'NO MATCH-PLATE/STICKER' then 1 else 0 end as NO_MATCH_PLATE_STICKER,\n",
    "    case when Violation = 'PLTFRM LFTS LWRD POS COMM VEH' then 1 else 0 end as PLTFRM_LFTS_LWRD_POS_COMM_VEH,\n",
    "    case when Violation = 'SAFETY ZONE' then 1 else 0 end as SAFETY_ZONE,\n",
    "    case when Violation = 'OBSTRUCTING DRIVEWAY' then 1 else 0 end as OBSTRUCTING_DRIVEWAY,\n",
    "    case when Violation = 'NO STOPPING-DAY/TIME LIMITS' then 1 else 0 end as NO_STOPPING_DAY_TIME_LIMITS,\n",
    "    case when Violation = 'COMML PLATES-UNALTERED VEHICLE' then 1 else 0 end as COMML_PLATES_UNALTERED_VEHICLE,\n",
    "    case when Violation = 'NGHT PKG ON RESID STR-COMM VEH' then 1 else 0 end as NGHT_PKG_ON_RESID_STR_COMM_VEH,\n",
    "    case when Violation = 'EXPIRED MUNI MTR-COMM MTR ZN' then 1 else 0 end as EXPIRED_MUNI_MTR_COMM_MTR_ZN,\n",
    "    case when Violation = 'OBSTRUCTING TRAFFIC/INTERSECT' then 1 else 0 end as OBSTRUCTING_TRAFFIC_INTERSECT,\n",
    "    case when Violation = 'NO STANDING-BUS LANE' then 1 else 0 end as NO_STANDING_BUS_LANE,\n",
    "    case when Violation = 'INSP STICKER-MUTILATED/CFEIT' then 1 else 0 end as INSP_STICKER_MUTILATED_C_FEIT,\n",
    "    case when Violation = 'STORAGE-3HR COMMERCIAL' then 1 else 0 end as STORAGE_3HR_COMMERCIAL,\n",
    "    case when Violation = 'NON-COMPLIANCE W/ POSTED SIGN' then 1 else 0 end as NON_COMPLIANCE_W_POSTED_SIGN,\n",
    "    case when Violation = 'NO STANDING-TAXI STAND' then 1 else 0 end as NO_STANDING_TAXI_STAND,\n",
    "    case when Violation = 'NO PARKING-EXC. AUTH. VEHICLE' then 1 else 0 end as NO_PARKING_EXC_AUTH_VEHICLE,\n",
    "    case when Violation = 'PEDESTRIAN RAMP' then 1 else 0 end as PEDESTRIAN_RAMP,\n",
    "    case when Violation = 'PARKED BUS-EXC. DESIG. AREA' then 1 else 0 end as PARKED_BUS_EXC_DESIG_AREA,\n",
    "    case when Violation = 'WRONG WAY' then 1 else 0 end as WRONG_WAY,\n",
    "    case when Violation = 'SELLING/OFFERING MCHNDSE-METER' then 1 else 0 end as SELLING_OFFERING_MCHNDSE_METER,\n",
    "    case when Violation = 'EXPIRED METER' then 1 else 0 end as EXPIRED_METER,\n",
    "    case when Violation = 'NO STANDING-HOTEL LOADING' then 1 else 0 end as NO_STANDING_HOTEL_LOADING,\n",
    "    case when Violation = 'REG STICKER-MUTILATED/CFEIT' then 1 else 0 end as REG_STICKER_MUTILATED_C_FEIT,\n",
    "    case when Violation = 'NO STANDING EXCP D/S' then 1 else 0 end as NO_STANDING_EXCP_D_S,\n",
    "    case when Violation = 'ANGLE PARKING' then 1 else 0 end as ANGLE_PARKING,\n",
    "    case when Violation = 'IMPROPER REGISTRATION' then 1 else 0 end as IMPROPER_REGISTRATION,\n",
    "    case when Violation = 'NO PARKING-EXC. HNDICAP PERMIT' then 1 else 0 end as NO_PARKING_EXC_HNDICAP_PERMIT,\n",
    "    case when Violation = 'TRAFFIC LANE' then 1 else 0 end as TRAFFIC_LANE,\n",
    "    case when Violation = 'BEYOND MARKED SPACE' then 1 else 0 end as BEYOND_MARKED_SPACE,\n",
    "    case when Violation = 'NO PARKING-EXC. HOTEL LOADING' then 1 else 0 end as NO_PARKING_EXC_HOTEL_LOADING,\n",
    "    case when Violation = 'MISSING EQUIPMENT' then 1 else 0 end as MISSING_EQUIPMENT,\n",
    "    case when Violation = 'FEEDING METER' then 1 else 0 end as FEEDING_METER,\n",
    "    case when Violation = 'OVERTIME PKG-TIME LIMIT POSTED' then 1 else 0 end as OVERTIME_PKG_TIME_LIMIT_POSTED,\n",
    "    case when Violation = 'DETACHED TRAILER' then 1 else 0 end as DETACHED_TRAILER,\n",
    "    case when Violation = 'TUNNEL/ELEVATED/ROADWAY' then 1 else 0 end as TUNNEL_ELEVATED_ROADWAY,\n",
    "    case when Violation = 'UNAUTHORIZED BUS LAYOVER' then 1 else 0 end as UNAUTHORIZED_BUS_LAYOVER,\n",
    "    case when Violation = 'NO PARKING-TAXI STAND' then 1 else 0 end as NO_PARKING_TAXI_STAND,\n",
    "    case when Violation = 'OTHER' then 1 else 0 end as OTHER,\n",
    "    case when Violation = 'VIN OBSCURED' then 1 else 0 end as VIN_OBSCURED,\n",
    "    case when Violation = 'IDLING' then 1 else 0 end as IDLING,\n",
    "    case when Violation = 'NO STD(EXC TRKS/GMTDST NO-TRK)' then 1 else 0 end as NO_STD_EXC_TRKS_GMTDST_NO_TRK,\n",
    "    case when Violation = 'OVERNIGHT TRACTOR TRAILER PKG' then 1 else 0 end as OVERNIGHT_TRACTOR_TRAILER_PKG,\n",
    "    case when Violation = 'INTERSECTION' then 1 else 0 end as INTERSECTION,\n",
    "    case when Violation = 'UNALTERED COMM VEHICLE' then 1 else 0 end as UNALTERED_COMM_VEHICLE,\n",
    "    case when Violation = 'FAILURE TO DISPLAY BUS PERMIT' then 1 else 0 end as FAILURE_TO_DISPLAY_BUS_PERMIT,\n",
    "    case when Violation = 'NO STANDING-FOR HIRE VEH STOP' then 1 else 0 end as NO_STANDING_FOR_HIRE_VEH_STOP,\n",
    "    case when Violation = 'BUS PARKING IN LOWER MANHATTAN' then 1 else 0 end as BUS_PARKING_IN_LOWER_MANHATTAN,\n",
    "    case when Violation = 'UNAUTHORIZED PASSENGER PICK-UP' then 1 else 0 end as UNAUTHORIZED_PASSENGER_PICK_UP,\n",
    "    case when Violation = 'NIGHTTIME STD/ PKG IN A PARK' then 1 else 0 end as NIGHTTIME_STD_PKG_IN_A_PARK,\n",
    "    case when Violation = 'EXCAVATION-VEHICLE OBSTR TRAFF' then 1 else 0 end as EXCAVATION_VEHICLE_OBSTR_TRAFF,\n",
    "    case when Violation = 'NO STANDING-COMMUTER VAN STOP' then 1 else 0 end as NO_STANDING_COMMUTER_VAN_STOP,\n",
    "    case when Violation = 'DIVIDED HIGHWAY' then 1 else 0 end as DIVIDED_HIGHWAY,\n",
    "    case when Violation = 'NO STOP/STANDNG EXCEPT PAS P/U' then 1 else 0 end as NO_STOP_STANDNG_EXCEPT_PAS_P_U,\n",
    "    case when Violation = 'OT PARKING-MISSING/BROKEN METR' then 1 else 0 end as OT_PARKING_MISSING_BROKEN_METR,\n",
    "    case when Violation = 'VEHICLE FOR SALE(DEALERS ONLY)' then 1 else 0 end as VEHICLE_FOR_SALE_DEALERS_ONLY,\n",
    "    case when Violation = 'NO OPERATOR NAM/ADD/PH DISPLAY' then 1 else 0 end as NO_OPERATOR_NAM_ADD_PH_DISPLAY,\n",
    "    case when Violation = 'ELEVATED/DIVIDED HIGHWAY/TUNNL' then 1 else 0 end as ELEVATED_DIVIDED_HIGHWAY_TUNNL,\n",
    "    case when Violation = 'ANGLE PARKING-COMM VEHICLE' then 1 else 0 end as ANGLE_PARKING_COMM_VEHICLE,\n",
    "    case when Violation = 'RAILROAD CROSSING' then 1 else 0 end as RAILROAD_CROSSING,\n",
    "    case when Violation = 'EXPIRED METER-COMM METER ZONE' then 1 else 0 end as EXPIRED_METER_COMM_METER_ZONE,\n",
    "    case when Violation = 'ALTERING INTERCITY BUS PERMIT' then 1 else 0 end as ALTERING_INTERCITY_BUS_PERMIT,\n",
    "    case when Violation = 'WASH/REPAIR VEHCL-REPAIR ONLY' then 1 else 0 end as WASH_REPAIR_VEHCL_REPAIR_ONLY,\n",
    "    case when Violation = 'PKG IN EXC. OF LIM-COMM MTR ZN' then 1 else 0 end as PKG_IN_EXC_OF_LIM_COMM_MTR_ZN,\n",
    "    case when Violation = 'VACANT LOT' then 1 else 0 end as VACANT_LOT,\n",
    "    case when Violation = 'NO STANDING EXCP DP' then 1 else 0 end as NO_STANDING_EXCP_DP,\n",
    "    case when Violation = 'MIDTOWN PKG OR STD-3HR LIMIT' then 1 else 0 end as MIDTOWN_PKG_OR_STD_3HR_LIMIT,\n",
    "    case when Violation = 'VEH-SALE/WSHNG/RPRNG/DRIVEWAY' then 1 else 0 end as VEH_SALE_WSHNG_RPRNG_DRIVEWAY,\n",
    "    case when Violation = 'MARGINAL STREET/WATER FRONT' then 1 else 0 end as MARGINAL_STREET_WATER_FRONT,\n",
    "    case when Violation = 'OVERTIME STDG D/S' then 1 else 0 end as OVERTIME_STDG_D_S,\n",
    "    case when Violation = 'BUS LANE VIOLATION' then 1 else 0 end as BUS_LANE_VIOLATION,\n",
    "    case when Violation = 'UNALTERED COMM VEH-NME/ADDRESS' then 1 else 0 end as UNALTERED_COMM_VEH_NME_ADDRESS,\n",
    "    case when Violation = 'NO STANDING-OFF-STREET LOT' then 1 else 0 end as NO_STANDING_OFF_STREET_LOT,\n",
    "    case when Violation = 'NO STANDING-SNOW EMERGENCY' then 1 else 0 end as NO_STANDING_SNOW_EMERGENCY,\n",
    "    case when Violation = 'REMOVE/REPLACE FLAT TIRE' then 1 else 0 end as REMOVE_REPLACE_FLAT_TIRE,\n",
    "    case when Violation = 'OVERTIME STANDING DP' then 1 else 0 end as OVERTIME_STANDING_DP,\n",
    "\n",
    "    `Fine Amount` as fine_amount\n",
    "    \n",
    "from \n",
    "    open_violations\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# query for model training dataset\n",
    "X_y = spark.sql(query)\n",
    "X_y.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490ac7c6-fa4b-444a-b786-a3fb2950c08f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o46.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# convert to dask - write training set to parquet\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mX_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtmp/regression_dataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1656\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o46.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "# convert to dask - write training set to parquet\n",
    "X_y.repartition(1).write.mode('overwrite').parquet('tmp/regression_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70f01e34-8443-426f-8bcf-5ddc289a36d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to dask - read from parquet to dask dataframe\n",
    "X_y = dd.read_parquet('/Users/maludee/repos/Big-Data-Project/notebooks/tmp/regression_dataset/*.parquet')  \n",
    "\n",
    "X = X_y[['in_state', 'passenger_car']].to_dask_array()\n",
    "y = X_y['fine_amount'].to_dask_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "911233a0-da3a-4780-902b-cae9664adcf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 736.37 MiB </td>\n",
       "                        <td> 736.37 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (96517058,) </td>\n",
       "                        <td> (96517058,) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 1 chunks in 3 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >96517058</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<values, shape=(96517058,), dtype=float64, chunksize=(96517058,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is needed to get the arrays to compute? \n",
    "X.compute_chunk_sizes()\n",
    "y.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8f6d226-50b3-408e-bdc1-f7a6486ed7a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a8b7b37-b0d0-497b-8cbe-a58a00a92887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get predictions\n",
    "yhat = lr.predict(X)\n",
    "\n",
    "# why is score returning nan?\n",
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8754d15d-fb55-44c5-93d7-4b4137f81230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 65.,  50.,  50.,  50.,  50., 115.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  65.,  50.,  45., 115.,  50.,  50.,  65.,  50.,\n",
       "        50.,  50.,  65.,  65.,  50.,  50.,  50.,  50.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:30].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f785b501-3531-4066-aed7-589adafb4aae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3496723 , -0.21716544,  2.07784501,  2.07784501,  2.07784501,\n",
       "        2.07784501,  2.07784501,  2.07784501,  2.07784501,  2.07784501,\n",
       "       -0.3496723 ,  2.07784501,  2.07784501,  2.07784501,  2.07784501,\n",
       "       -0.3496723 ,  2.07784501,  2.07784501,  2.07784501,  2.07784501,\n",
       "        2.07784501,  2.07784501,  2.07784501, -0.21716544, -0.3496723 ,\n",
       "       -0.3496723 ,  2.07784501,  2.07784501,  2.07784501, -0.3496723 ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[:30].compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4245f6-886e-49db-a04d-c34988e19552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
